{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental analysis\n",
    "\n",
    "This notebook provides the software required to perform the experimentation that accompanies the following article:\n",
    "\n",
    "    A Genetic Clustering Approach to Extract Data from HTML Tables\n",
    "\tPatricia Jiménez, Juan C. Roldán, Rafael Corchuelo\n",
    "\tSubmitted to Information Processing & Management\n",
    "\n",
    "Prior to running it, you must create a conda environment that hosts a Python 3.7 interpreter and then install the dependencies in file \"requirements.txt\".\n",
    "\n",
    "Once your environment is ready, please, go ahead!  Just execute the following cells one after the other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation\n",
    "\n",
    "Please, run the following cell to set the experimentation environment up.  Don't worry if you get some deprecation warnings regarding TensorFlow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention_decoder import AttentionDecoder\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from keras import Sequential\n",
    "from keras.layers import Reshape, LSTM, Convolution2D, BatchNormalization, Dense, Flatten\n",
    "from numpy import array, argmax\n",
    "from os import listdir\n",
    "from pickle import load\n",
    "from pprint import pprint\n",
    "from random import sample, seed, shuffle\n",
    "from regex import sub\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score\n",
    "from statistics import stdev, mean\n",
    "\n",
    "from tablextract import segmentate, functional_analysis, structural_analysis\n",
    "from tablextract_melva import segmentate as melva_segmentate, functional_analysis as melva_functional_analysis, structural_analysis as melva_structural_analysis\n",
    "\n",
    "from time import process_time as cpu_time\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now set a few configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set \"SAMPLE\" to True to perform a quick experimentation on a small subset of tables.\n",
    "SAMPLE = False\n",
    "\n",
    "# Set \"FOLDS\" to the number of folds to used during k-fold cross validation.\n",
    "FOLDS = 3\n",
    "\n",
    "# Set \"SEED\" to an arbitrary string to reproduce the results.  None means total randomness.\n",
    "\n",
    "#SEED = 'TOMATE301219'\n",
    "SEED = None\n",
    "\n",
    "# Set \"PATH_RESULTS\" to the CSV file where the results will be output.  \n",
    "# Include a \"%s\" that will be replaced by the name of each proposal at runtime\n",
    "PATH_RESULTS = './output/results_%s.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load the tables and compute their features.  For performance purposes, we only keep the features used by the proposals that are compared in this experimental analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = load_tables(function_names=FUNCTION_NAMES_EXPERIMENTATION)\n",
    "keep_features = ('tag', 'colspan', 'rowspan', 'relative-row', 'relative-col', 'background-color-r', 'background-color-g', 'background-color-b', 'font-family')\n",
    "for table in tables:\n",
    "    with open(PATH_ORIGINAL_TABLE % table['_id'], 'rb') as fp:\n",
    "        table['features'] = [\n",
    "            [\n",
    "                {\n",
    "                    feat: cell[feat]\n",
    "                    for feat in keep_features\n",
    "                    if feat in cell\n",
    "                }\n",
    "                for cell in row\n",
    "            ] for row in load(fp).features\n",
    "        ]\n",
    "init_num_tables = len(tables)\n",
    "tables = [\n",
    "    t for t in tables\n",
    "    if len([cell for row in t['features'] for cell in row]) == len([cell for row in t['functions'] for cell in row]) == len([cell for row in t['texts'] for cell in row])\n",
    "]\n",
    "final_num_tables = len(tables)\n",
    "\n",
    "print('There are %s annotated tables in the repository.' % init_num_tables)\n",
    "print('There are %s fully-annotated tables.' % final_num_tables)\n",
    "print('A total of %s partially-annotated tables were ignored.' % (init_num_tables - final_num_tables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's shuffle the tables, split the datasets into Wikipedia and DWTC, and split them into folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(SEED)\n",
    "shuffle(tables)\n",
    "\n",
    "if SAMPLE:\n",
    "    corpuses = [t for t in tables if t['kind'] == 'horizontal listing'][:40]\n",
    "    corpuses.extend([t for t in tables if t['kind'] == 'vertical listing'][:30])\n",
    "    corpuses.extend([t for t in tables if t['kind'] == 'matrix'][:30])\n",
    "    corpuses = {'Sample': corpuses}\n",
    "else:    \n",
    "    corpuses = {\n",
    "        'Wikipedia': [t for t in tables if 'en.wikipedia' in t['document_id']],\n",
    "        'DWTC': [t for t in tables if 'en.wikipedia' not in t['document_id']]\n",
    "    }\n",
    "\n",
    "print('Tables by dataset:\\n%s' % '\\n'.join('   %s: %s' % (k, len(v)) for k, v in corpuses.items()))\n",
    "print('Cells by dataset:\\n%s' % '\\n'.join('   %s: %s' % (k, sum(sum(len(row) for row in t) for t in v)) for k, v in corpuses.items()))\n",
    "\n",
    "for c in corpuses:\n",
    "    fold_size = int(len(corpuses[c]) / FOLDS)\n",
    "    corpuses[c] = [corpuses[c][n:n + fold_size] for n in range(0, len(corpuses[c]), fold_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation method\n",
    "\n",
    "This method performs the evaluation.  Given a proposal to evaluate (extractor), it runs it and computes the experimental results, namely: precision, recall, F_1, average number and deviation of CPU second per table.  The measures are computed using k-fold-cross validation.  (The value of k is taken from variable \"FOLDS\", which was set above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(extractor, label, multitable_prediction=False):\n",
    "    rows = [['CORPUS', 'TABLE_ID', 'TABLE_KIND', 'P', 'R', 'F_1', 'ACCURACY', 'TRAIN_TIME', 'PREDICT_TIME']]\n",
    "    for c in corpuses:\n",
    "        print(c)\n",
    "        for f in range(FOLDS):\n",
    "            print(f\"Fold #{f + 1}/{FOLDS}\")\n",
    "            # get the train and test folds\n",
    "            train = [table for n, fold in enumerate(corpuses[c]) for table in fold if n != f]\n",
    "            test = corpuses[c][f]\n",
    "            # instantiate and train the extractor\n",
    "            ext = extractor()\n",
    "            seed(SEED)\n",
    "            train_time = cpu_time()            \n",
    "            ext.train(train)            \n",
    "            train_time = (cpu_time() - train_time) / len(train)\n",
    "            # test the extractor\n",
    "            if multitable_prediction:\n",
    "                seed(SEED)\n",
    "                predict_time = cpu_time()                \n",
    "                predicted_tables, discount_time = ext.predict(test)                \n",
    "                predict_time = (cpu_time() - predict_time - discount_time) / len(test)\n",
    "                for table, predicted_table in zip(test, predicted_tables):\n",
    "                    annotated = [cell for row in table['functions'] for cell in row]\n",
    "                    predicted = [cell for row in predicted_table for cell in row]\n",
    "                    #if len(set(predicted)) == 1: print(f\"ONLY ONE CLUSTER {table['_id']}\")\n",
    "                    p, r, f_1, _ = precision_recall_fscore_support(annotated, predicted, average='macro', labels=('data', 'meta-data'), zero_division=0)\n",
    "                    accuracy = accuracy_score(annotated, predicted)\n",
    "                    #roc_auc = roc_auc_score(annotated, predicted, average='macro')\n",
    "                    rows.append([c, table['_id'], table['kind'], p, r, f_1, accuracy, train_time, predict_time])\n",
    "            else:\n",
    "                for table in test:\n",
    "                    annotated = [cell for row in table['functions'] for cell in row]\n",
    "                    predict_time = cpu_time()\n",
    "                    try:\n",
    "                        predicted_table, discount_time = ext.predict(table)\n",
    "                        predicted = [cell for row in predicted_table for cell in row]\n",
    "                    except:\n",
    "                        print(f\"EXCEPTION OCCURRED TABLE {table['_id']}\")\n",
    "                        print_exc()\n",
    "                        discount_time = 0\n",
    "                        predicted = ['failed' for _ in range(len(annotated))]\n",
    "                    predict_time = cpu_time() - predict_time - discount_time\n",
    "                    #if len(set(predicted)) == 1: print(f\"ONLY ONE CLUSTER {table['_id']}\")\n",
    "                    p, r, f_1, _ = precision_recall_fscore_support(annotated, predicted, average='macro', labels=('data', 'meta-data'), zero_division=0)\n",
    "                    accuracy = accuracy_score(annotated, predicted)\n",
    "                    #roc_auc = roc_auc_score(annotated, predicted, average='macro')\n",
    "                    rows.append([c, table['_id'], table['kind'], p, r, f_1, accuracy, train_time, predict_time])\n",
    "    # export the evaluation result\n",
    "    with open(PATH_RESULTS % label, 'w') as fp:\n",
    "        fp.write('\\n'.join(\n",
    "            '\\t'.join(map(str, row))\n",
    "            for row in rows\n",
    "        ))\n",
    "    # print summary results\n",
    "    for c in corpuses:\n",
    "        print(c)\n",
    "        corpus_rows = [row for row in rows[1:] if row[0] == c]\n",
    "        for kind in ('overall', 'horizontal listing', 'vertical listing', 'matrix'):\n",
    "            kind_rows = [row for row in corpus_rows if row[2] == kind or kind == 'overall']\n",
    "            p = mean([row[3] for row in kind_rows])\n",
    "            r = mean([row[4] for row in kind_rows])\n",
    "            f_1 = mean([row[5] for row in kind_rows])\n",
    "            accuracy = mean([row[6] for row in kind_rows])\n",
    "            time_avg = mean([row[7] + row[8] for row in kind_rows])\n",
    "            time_dev = stdev([row[7] + row[8] for row in kind_rows])\n",
    "            print(\n",
    "                '\\t%s: p = %.2f, r = %.2f, f_1 = %.2f, acc = %.2f, time_avg = %.2f, time_dev = %.2f'\n",
    "                %\n",
    "                (kind.rjust(25), 100 * p, 100 * r, 100 * f_1, 100 * accuracy, time_avg, time_dev)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yoshida et al.\n",
    "\n",
    "The following cell implements Yoshida et al.'s proposal and evaluates it.  Note that the cell produces some output during the evaluation.  The results are stored in the file specified by variable \"PATH_RESULTS\", which was defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class YoshidaExtractor:\n",
    "    \n",
    "    layouts = ((0, None), (1, 'h'), (1, 'v'), (2, 'h'), (3, 'h'), (4, 'h'), (2, 'v'), (3, 'v'), (4, 'v'))\n",
    "\n",
    "    def train(self, tables):\n",
    "        pass\n",
    "\n",
    "    def predict(self, tables):\n",
    "        knds = self.kinds(tables)\n",
    "        res = []\n",
    "        for table, (period, orientation) in zip(tables, knds):\n",
    "            pred_table = [\n",
    "                [\n",
    "                    'data' if self.get_function(period, orientation, r, c) == 0 else 'meta-data'\n",
    "                    for c in range(len(row))\n",
    "                ] for r, row in enumerate(table['texts'])\n",
    "            ]\n",
    "            res.append(pred_table)\n",
    "        return res, 0\n",
    "\n",
    "    def kinds(self, tables, iterations=10, repetitions=100):\n",
    "        T = [table['texts'] for table in tables]\n",
    "        best_score = -1\n",
    "        best_M = None\n",
    "        for _ in range(repetitions):\n",
    "            M = [sample(self.layouts, 1)[0] for _ in range(len(tables))]\n",
    "            for i in range(iterations):\n",
    "                probs = self.compute_probs(T, M)\n",
    "                M, score = self.compute_layouts(T, probs)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_M = M\n",
    "        return best_M\n",
    "\n",
    "    def get_function(self, period, orientation, r, c):\n",
    "        if orientation == None:\n",
    "            res = 0\n",
    "        else:\n",
    "            i = r if orientation == 'h' else c\n",
    "            if period == 1:\n",
    "                res = 1 if r == 0 else 0\n",
    "            elif period == 4:\n",
    "                res = 1 if r == 1 else 0\n",
    "            else:\n",
    "                res = 1 if r % period == 0 else 0\n",
    "        return res\n",
    "\n",
    "    def compute_probs(self, T, M):\n",
    "        ''' Given a list of tables and their type, return a dictionary with cell\n",
    "        text as keys and probability of them being data as values. '''\n",
    "        def add_occurrence(dct, text, kind):\n",
    "            if text not in dct: dct[text] = [0, 0]\n",
    "            dct[text][kind] += 1\n",
    "        res = {}\n",
    "        for table, (period, orientation) in zip(T, M):\n",
    "            for r, row in enumerate(table):\n",
    "                for c, cell in enumerate(row):\n",
    "                    add_occurrence(res, cell, self.get_function(period, orientation, r, c))\n",
    "        return {k: (v[0] / (v[0] + v[1]), v[1] / (v[0] + v[1])) for k, v in res.items()}\n",
    "\n",
    "    def compute_layouts(self, T, probs):\n",
    "        res = []\n",
    "        total_score = 0\n",
    "        for table in T:\n",
    "            max_score = -1\n",
    "            max_type = None\n",
    "            for period, orientation in self.layouts:\n",
    "                score = 0\n",
    "                for r, row in enumerate(table):\n",
    "                    for c, cell in enumerate(row):\n",
    "                        kind = self.get_function(period, orientation, r, c)\n",
    "                        score += probs[cell][kind]\n",
    "                if score > max_score:\n",
    "                    max_score = score\n",
    "                    max_type = (period, orientation)\n",
    "            res.append(max_type)\n",
    "            total_score += max_score / (len(table) * len(table[0]))\n",
    "        return res, total_score\n",
    "\n",
    "evaluate(YoshidaExtractor, 'yoshida', multitable_prediction=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embley\n",
    "\n",
    "\n",
    "The following cell implements Embley et al.'s proposal and evaluates it. Note that the cell produces some output during the evaluation.  The results are stored in the file specified by variable \"PATH_RESULTS\", which was defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbleyExtractor:\n",
    "    def train(self, tables):\n",
    "        pass\n",
    "\n",
    "    def predict(self, table):\n",
    "        res = self.functions(table['texts'])\n",
    "        return [['data' if cell == 0 else 'meta-data' for cell in row] for row in res], 0\n",
    "        \n",
    "    def functions(self, table):\n",
    "        CC1, CC2 = self.mips(table)\n",
    "        res = []\n",
    "        for r, row in enumerate(table, 1):\n",
    "            res.append([])\n",
    "            for c, cell in enumerate(row, 1):\n",
    "                if CC1[0] <= r <= CC2[0] or CC1[1] <= c <= CC2[1]:\n",
    "                    res[-1].append(1)\n",
    "                else:\n",
    "                    res[-1].append(0)\n",
    "        return res\n",
    "\n",
    "    def no_duplicates(self, table, r1, c1, r2, c2, kind='rows', empty_matters=False):\n",
    "        tab = [row[c1 - 1: c2] for row in table[r1 - 1: r2]]\n",
    "        if empty_matters: return False\n",
    "        if kind == 'cols': tab = [*zip(*tab)]  # tricky list trasposition\n",
    "        tab = [tuple(row) for row in tab]\n",
    "        return len(tab) == len(set(tab))\n",
    "\n",
    "    def mips(self, table):\n",
    "        # initialize\n",
    "        CC1, CC2 = None, (1, 1)\n",
    "        C_max, R_max = len(table[0]), len(table)  # CC4 is trivial in web tables\n",
    "        R_1 = 1; C_1 = 1; R_2 = R_max - 1; C_2 = 1\n",
    "        rightflag = upflag = 0\n",
    "        max_area = 0\n",
    "\n",
    "        # locate candidate MIPs by finding the minimum indexing headers\n",
    "        while C_2 < C_max and R_2 >= R_1:\n",
    "            if self.no_duplicates(table, R_2 + 1, C_1, R_max, C_2, 'rows') and self.no_duplicates(table, R_1, C_2 + 1, R_2 - 1, C_max, 'cols'):\n",
    "                R_2 -= 1\n",
    "                upflag = 1; rightflag = 0\n",
    "            else:\n",
    "                C_2 += 1\n",
    "                rightflag = 1\n",
    "                if upflag == rightflag == 1:\n",
    "                    data_area = (R_max - R_2 + 1) * (C_max - C_2 + 1)\n",
    "                    if data_area > max_area:\n",
    "                        max_area = data_area\n",
    "                        CC2 = (R_2, C_2)\n",
    "                    upflag = 0\n",
    "\n",
    "        # locate CC1 at intersection of the top row and the leftmost column necessary for indexing\n",
    "        R_1, C_1 = 1, 1\n",
    "        while self.no_duplicates(table, R_1 + 1, C_2 + 1, R_2, C_max, 'cols', True): R_1 += 1\n",
    "        while self.no_duplicates(table, R_2 + 1, C_1 + 1, C_2, R_max, 'rows', True): C_1 += 1\n",
    "        return (R_1, C_1), CC2\n",
    "\n",
    "evaluate(EmbleyExtractor, 'embley')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jung and Kwon\n",
    "\n",
    "The following cell implements Jung and Kwon's proposal and evaluates it. Note that the cell produces some output during the evaluation.  The results are stored in the file specified by variable \"PATH_RESULTS\", which was defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JungExtractor:\n",
    "    def train(self, tables):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, table):\n",
    "        res = self.functions(table)\n",
    "        return [['data' if cell == 0 else 'meta-data' for cell in row] for row in res], 0\n",
    "        \n",
    "    def functions(self, table):\n",
    "        w = len(table['texts'][0])\n",
    "        h = len(table['texts'])\n",
    "        heurs = [[[-1 for c in range(w)] for r in range(h)] for _ in range(7)]\n",
    "\n",
    "        # cell property heuristics: 5.1 and 5.6\n",
    "        for r, (row_text, row_feats) in enumerate(zip(table['texts'], table['features'])):\n",
    "            for c, (cell_text, cell_feats) in enumerate(zip(row_text, row_feats)):\n",
    "                if 'tag' in cell_feats:\n",
    "                    # heuristic 5.1\n",
    "                    heurs[0][r][c] = int(cell_feats['tag'] == 'th')\n",
    "                    # heuristic 5.6\n",
    "                    heurs[5][r][c] = int((cell_feats['colspan'] * w > 1 or cell_feats['rowspan'] * h > 1) and (cell_feats['relative-row'] == 0 or cell_feats['relative-col'] == 0))\n",
    "                else:\n",
    "                    heurs[0][r][c] = 0\n",
    "                    heurs[5][r][c] = 0\n",
    "\n",
    "        # heuristics 5.2\n",
    "        bgcols = [\n",
    "            [\n",
    "                (int(255 * cell['background-color-r']), int(255 * cell['background-color-g']), int(255 * cell['background-color-b'])) if 'tag' in cell else 0\n",
    "                for cell in row\n",
    "            ] for row in table['features']\n",
    "        ]\n",
    "        palette = list(set(bgcol for row in bgcols for bgcol in row))\n",
    "        if len(palette) == 2:\n",
    "            pos = {p: 0 for p in palette}\n",
    "            for r in range(h):\n",
    "                for c in range(w):\n",
    "                    pos[bgcols[r][c]] += c + r\n",
    "            header_color = min(palette, key=lambda x: pos[x])\n",
    "            for r in range(h):\n",
    "                for c in range(w):\n",
    "                    heurs[1][r][c] = 1 if bgcols[r][c] == header_color else 0\n",
    "\n",
    "        # heuristics 5.3\n",
    "        bgcols = [\n",
    "            [\n",
    "                cell['font-family']\n",
    "                for cell in row if 'tag' in cell\n",
    "            ] for row in table['features']\n",
    "        ]\n",
    "        palette = list(set(bgcol for row in bgcols for bgcol in row))\n",
    "        if len(palette) == 2:\n",
    "            pos = {p: 0 for p in palette}\n",
    "            for r in range(min(h, len(bgcols))):\n",
    "                for c in range(min(w, len(bgcols[r]))):\n",
    "                    pos[bgcols[r][c]] += c + r\n",
    "            header_color = min(palette, key=lambda x: pos[x])\n",
    "            for r in range(min(h, len(bgcols))):\n",
    "                for c in range(min(w, len(bgcols[r]))):\n",
    "                    heurs[1][r][c] = 1 if bgcols[r][c] == header_color else 0\n",
    "\n",
    "        # heuristic 5.5\n",
    "        def patternise(text):\n",
    "            return sub(r'\\(.+\\)', 'I', sub(r'\\d+', 'D', sub(r'[\\p{L}]+', 'W', text)))\n",
    "\n",
    "        for r in range(h):\n",
    "            row = [patternise(c) for c in table['texts'][r]]\n",
    "            if len(set(row[1:])) == 1:\n",
    "                heurs[4][r][0] = 1\n",
    "\n",
    "        for c in range(w):\n",
    "            col = [patternise(row[c]) for row in table['texts']]\n",
    "            if len(set(col[1:])) == 1:\n",
    "                heurs[4][0][c] = 1\n",
    "\n",
    "        if any(c == 1 for row in heurs[4] for c in row):\n",
    "            for r in range(h):\n",
    "                for c in range(w):\n",
    "                    if heurs[4][r][c] == -1: heurs[4][r][c] = 0\n",
    "\n",
    "        # table heuristics: 5.7\n",
    "        if any(len(t) == 0 for t in table['texts'][0]):\n",
    "            for c in range(w): heurs[6][0][c] = 1\n",
    "        if any(len(row[0]) == 0 for row in table['texts']):\n",
    "            for r in range(h): heurs[6][r][0] = 1\n",
    "        if any(c == 1 for row in heurs[6] for c in row):\n",
    "            for r in range(h):\n",
    "                for c in range(w):\n",
    "                    if heurs[6][r][c] == -1: heurs[6][r][c] = 0\n",
    "\n",
    "        weights = [.125, .188, .125, .125, .0625, .1875, .187]\n",
    "        to_ignore = []\n",
    "        for n, heur in enumerate(heurs):\n",
    "            if heur[0][0] == -1: to_ignore.append(n)\n",
    "        for n in reversed(to_ignore):\n",
    "            weights.pop(n)\n",
    "            heurs.pop(n)\n",
    "\n",
    "        weights = [w / sum(weights) for w in weights]\n",
    "        functions = [[int(round(sum(w * heur[r][c] for w, heur in zip(weights, heurs)))) for c in range(w)] for r in range(h)]\n",
    "        return functions\n",
    "\n",
    "evaluate(JungExtractor, 'jung')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nishida et al.\n",
    "\n",
    "The following cell implements Nishida et al.'s proposal and evaluates it. Note that the cell produces some output during the evaluation.  The results are stored in the file specified by variable \"PATH_RESULTS\", which was defined above.  Please, note that this proposal relies on TensorFlow, which issues several deprecation warnings that you may safely ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NishidaExtractor:\n",
    "    def __init__(self):\n",
    "        model = Sequential()\n",
    "        model.add(Reshape((64, 50 * 100)))\n",
    "        model.add(LSTM(100, return_sequences=True))\n",
    "        model.add(AttentionDecoder(100, 100))\n",
    "        model.add(Reshape((8, 8, 100)))\n",
    "        model.add(Convolution2D(32, (3, 3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Convolution2D(32, (3, 3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(3))\n",
    "        model.compile(optimizer='SGD', loss='mean_squared_error')\n",
    "        self.model = model\n",
    "        self.batch_size = 5 if SAMPLE else 50\n",
    "        self.epochs = 8 if SAMPLE else 80\n",
    "\n",
    "    def train(self, tables):\n",
    "        ids = [NISHIDA_IDS.index(t['_id']) for t in tables]\n",
    "        self.model.fit(x=NISHIDA_X[ids], y=NISHIDA_Y[ids], batch_size=self.batch_size, epochs=self.epochs)\n",
    "\n",
    "    def predict(self, table):\n",
    "        rows, cols = len(table['texts']), len(table['texts'][0])\n",
    "        tab = NISHIDA_X[NISHIDA_IDS.index(table['_id'])]\n",
    "        cls = self.model.predict(array([tab]))\n",
    "        cls = argmax(cls)\n",
    "        res = [['data' for _ in range(cols)] for _ in range(rows)]\n",
    "        if cls == 0 or cls == 2:\n",
    "            res[0] = ['meta-data' for _ in range(cols)]\n",
    "        if cls == 1 or cls == 2:\n",
    "            for r in range(rows):\n",
    "                res[r][0] = 'meta-data'\n",
    "        return res, 0\n",
    "\n",
    "with open('../DATA/nishida/names.pk', 'rb') as fp:\n",
    "    NISHIDA_IDS = load(fp)\n",
    "with open('../DATA/nishida/tables.pk', 'rb') as fp:\n",
    "    NISHIDA_X = load(fp)\n",
    "with open('../DATA/nishida/kinds.pk', 'rb') as fp:\n",
    "    NISHIDA_Y = load(fp)\n",
    "    \n",
    "evaluate(NishidaExtractor, 'nishida')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melva\n",
    "\n",
    "The following cell implements our proposal and evaluates it. Note that the cell produces some output during the evaluation.  The results are stored in the file specified by variable \"PATH_RESULTS\", which was defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from WorkerManager import WorkerManager\n",
    "from DiskCache import DiskCache\n",
    "\n",
    "WorkerManager.initialise()\n",
    "\n",
    "class TomateMelvaExtractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        normalization='min-max-global',  # The choices are \"min-max-global\", \"min-max-local\", \"standard\", or \"softmax\"\n",
    "        clustering_features=['style', 'syntax', 'structural', 'semantic'],  # any subset of these values is valid for experimentation.\n",
    "        dimensionality_reduction='off',  # \"off\" is the only choice with Melva, since melva implements its own dim. red. procedure.\n",
    "        clustering_method='melva' # \"melva\" is the only choice now.\n",
    "    ):\n",
    "        self.normalization = normalization\n",
    "        self.clustering_features = clustering_features\n",
    "        self.dimensionality_reduction = dimensionality_reduction\n",
    "        self.clustering_method = clustering_method\n",
    "        \n",
    "    def train(self, tables):\n",
    "        DiskCache.initialise()        \n",
    "    \n",
    "    def predict(self, table):\n",
    "        DiskCache.initialise()\n",
    "        discount_time = cpu_time()\n",
    "        with open(PATH_ORIGINAL_TABLE % table['_id'], 'rb') as fp:\n",
    "            source = load(fp)\n",
    "        source.element = soup(source.element)\n",
    "        melva_segmentate(source, add_image_text=True, add_link_urls=False, base_url=source.url, normalization=self.normalization, text_metadata_dict=METADATA_CORPUS)\n",
    "        discount_time = cpu_time() - discount_time\n",
    "        \n",
    "        melva_functional_analysis(source, self.clustering_features, self.dimensionality_reduction, self.clustering_method)\n",
    "        melva_structural_analysis(source)\n",
    "        predicted_table = [[FUNCTION_NAMES_PREDICTION[cell] for cell in row] for row in source.functions]\n",
    "        \n",
    "        return predicted_table, discount_time\n",
    "\n",
    "evaluate(TomateMelvaExtractor, \"melva\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomate",
   "language": "python",
   "name": "tomate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
